## Архитектура HDFS

Основана на архитектуре GFS(Google File System). Является её опен-сорсным аналогом под лицензией Apache.

- Работает на кластере серверов;
- Для пользователя выглядит как один диск, т.е. hdfs сам по себе распределяет, реплецирует и хранит файлы;
- Построена поверх обычных linux файловых систем([[ext4]], [[btrfs]], [[zfs]]).

<mark style="background: #FFF3A3A6;">Fault tolerant</mark>, т.е. данные не теряются если диск или сервер выходит из строя.
Железо, для HDFS - это не суперкомпьютеры, а обычное серверное железо.

HDFS хорошо подходит для того, чтобы хранить файлы по несколько петтабайт, но лучше хранить большие файлы, чем множество маленьких файлов.
HDFS подходит для:
- Паттерн <mark style="background: #FFF3A3A6;">"write once, read many times"</mark>, т.е. лучше создавать логику работы с HDFS таким образом, чтобы записывать в файл данные один раз, а использовать эти данные много раз;
- <mark style="background: #FFF3A3A6;">Оптимизация под последовательное чтение</mark>, т.е. нельзя начать читать файл в произвольном месте, т.к. придется переставлять головку жёсткого диска много раз;
- Опперация append появилась в Hadoop 0.21.
HDFS <mark style="background: #FF5582A6;">не подходит</mark>:
- <mark style="background: #FF5582A6;">Low latency reads</mark>
	- Высокая пропускная способностьь вместо быстрого доступа к данным, т.е. если нам необходимо получать маленькое количество данных очень быстро, то использовать HDFS в данном случае не оптимально, т.к. HDFS сконструирован под чтение больших файлов и обработки больших объёмов данных единовременно;
	- [[Hbase]] частично помогает решить данную задачу.
- <mark style="background: #FF5582A6;">Большое количество маленьких файлов.</mark> HDFS архитектурно устроена таким образом, что лучше хранить один большой файл на 1000МБ, чем 1000 маленьких файлов по 1МБ;
- Многопоточная запись:
	- Один процесс записи на файл. Т.е. нет возможности 2 процессам/потокам писать в один файл;
	- Данные дописываются в конец файла;
	- Нет поддержки записи по смещению.
##  Демоны HDFS

[[Демон]] - это <mark style="background: #FFF3A3A6;">фоновый процесс</mark>, который запускается вместе с запуском системы. Он работает в фоне без взаимодействия с пользователем и <mark style="background: #FFF3A3A6;">обеспечивает</mark> в целом <mark style="background: #FFF3A3A6;">работу системы</mark>.
В HDFS существует 3 типа daemon процессов:
- NameNode
- Secondary NameNode
- DataNode

![[Pasted image 20250603145821.png]]

Распишем про каждый из демонов поконкретнее.

### NameNode

NameNode - <mark style="background: #FFF3A3A6;">это основной процесс в HDFS.</mark> Он запускается на одной выделенной машине. Этот процесс отвечает за:
- файловое пространство(namespace)
- мета-информацию
- расположение блока файлов
<mark style="background: #FFF3A3A6;">NameNode</mark> практически не хранит данные у себя, она <mark style="background: #FFF3A3A6;">хранит только мета-информацию о данных.</mark> Т.е. он хранит информацию, где какие файлы находятся, т.е. собирает информацию о дереве HDFS. Сервер, на котором крутится NameNode должен быть достаточно большим с большим количеством оперативной памяти, т.к. <mark style="background: #FFF3A3A6;">вся информация о файлах хранится в оперативной памяти</mark>. Соответственно, <mark style="background: #FF5582A6;">количество файлов</mark> в кластере напрямую <mark style="background: #FF5582A6;">ограничено количеством оперативной памяти</mark> в NameNode.
NameNode должен быть отказоустойчивым, т.к. при падении NameNode мы теряем возможность взаимодействовать со всем кластером.

### DataNode

DataNode - это демон отвечающий за хранение информации.
- Отдаёт и принимает блоки данных;
- Отправляет отчёт о состоянии на NameNode;
- Запускается на каждой машине кластера.

### Secondary NameNode

NameNode хранит свое состояние в файле <mark style="background: #FF5582A6;">fsimage</mark>. И в дальнейшем в эксплуатации добавляет изменения <mark style="background: #FFF3A3A6;">в log файл</mark>. Проблема в том, что когда падает NameNode, чтобы восстановить работу HDFS необходимо время, чтобы она накатила изменения из log файла. Для этого и существует Secondary NameNode, которая:
- <mark style="background: #FF5582A6;">Периодически обновляет свой fsimage</mark>, т.е. при поднятии не нужно накатывать изменения из log файла;
- Требует то же железо, что и NameNode;
- <mark style="background: #FF5582A6;">(!) Не используется для high-availability, т.е. это не backup для NameNode</mark>.

## Файлы и блоки

<mark style="background: #FFF3A3A6;">Файлы в HDFS состоят из блоков.</mark> Соответственно блок это единица хранения данных. <mark style="background: #FFF3A3A6;">Информация о местонахождении</mark> конкретных блоков, конкретного файла находится на <mark style="background: #FFF3A3A6;">NameNode</mark>. Соответсвенно сами блоки хранятся на DataNode. DataNode не знает частью какого файла является конкретный блок.Чтобы прочитать файл процесс сначала идёт к NameNode, узнаёт из каких блоков состоит файл и где эти блоки находятся. 

### Репликация

Блоки <mark style="background: #FFF3A3A6;">реплицируются</mark> по машинам <mark style="background: #FFF3A3A6;">в процессе записи</mark>:
 - Один и тот же блок хранится на нескольких DataNode;
 - Фактор репликации по умолчанию равен 3;
 - Это необходимо для fault-tolerance и упрощению доступа.

Например, у нас упал один сервер и нам необходимо прочитать какие-то данные с упавшего сервера. Если бы у нас не было репликации, то данные были бы в лучшем случае задержаны для получения, а в худшем случае и вовсе утеряны. <mark style="background: #FFF3A3A6;">Соответственно с помощью репликации мы не теряем данные, даже если упал целый сервер</mark>, потому что у нас есть ещё как минимум 2 таких же блока, которые находятся на других DataNode для каждого из блоков находящихся на упавшем DataNode. 

<mark style="background: #FFF3A3A6;">Так же данные реплики могут быть использованы для упрощения доступа к данным</mark>. Например 2-ум процессам необходим доступ к одним и тем же данным на чтение. Для этого можно использовать <mark style="background: #FFF3A3A6;">одни и те же блоки с разных DataNode,</mark> за счёт чего мы увеличим скорость чтения данных для обоих процессов, т.к. второму процессу не придется ждать пока первый процесс закончится.

<mark style="background: #FFF3A3A6;">Фактор репликации тоже можно варьировать.</mark> Если у нас хранятся особенно важные данные, то можно повысить фактор репликации до 4 или до 5. С этим не стоит переусердствовать, <mark style="background: #FFF3A3A6;">т.к. повышение фактора репликации приводит к понижению количества файлов, которые можно записать в HDFS.</mark>
<mark style="background: #FF5582A6;">Если же у нас хранятся не особенно важные данные,</mark> которые можно восстановить из других источников, то <mark style="background: #FF5582A6;">можно понизить фактор репликации до 2.</mark> До 1 никто обычно не понижает фактор репликации, потому что каждый отказ приводит к потере данных, из-за чего придется часто использовать механизмы восстановления.

![[Pasted image 20250603161127.png]]

<mark style="background: #FFF3A3A6;">Стандартный размер блоков в HDFS 64МБ, 128МБ, 256МБ.</mark>
<mark style="background: #FF5582A6;">Такие большие блоки необходимы для того, чтобы seek time(время перемещения головки диска в место, с которого необходимо начать считывание) был значительно меньше transfer rate(времени считывания).</mark>
На картинке внизу ярко представлена разница в том, что если у нас будет большое кол-во маленьких блоков, мы потеряем время на сдвиги головки и из-за этого увеличиться время чтения одного файла.

![[Pasted image 20250603161515.png]]

Репликацией блоков занимается NameNode. При этом необходимо балансировать между надёжностью и нагрузкой на сеть:
- Попытка снизить нагрузку на сеть(bandwidth)
- Попытка улучшить надежность путём размещения реплик в разных стойках.
	- Из трёх реплик первую рамещают на DataNode
	- Вторую реплику размещают в той же стойке но на другой DataNode
	- Третью реплику размещают на DataNode в другой стойке
Это делают для того, чтобы если сломается даже вся стойка, данные всё равно можно было бы восстановить.

## Взаимодействие клиента hdfs  и его демонов

Клиентами для демонов могут выступать различные Java/Python программы, так же это может быть shell команды. 
Все действия клиента можно разделить на 2 группы:
- получение данных. Например, в какой директории находится файл, на сколько блоков он разбит, на какие кластера реплецирован и т.д.
- запись данных. Можно менять права доступа к файлу, фактор репликации файла и т.д.
Процесс чтения файла:
1. Сначала клиент запрашивает у NameNode расположение блоков B1 и B2
2. После получения информации о местоположении блоков клиент начинает вычитывать данные с DataNode
   ![[Pasted image 20250603193142.png]]
3. Если при чтении произошла ошибка и блок B2 не был прочитан, то, т.к. у нас есть репликация, можно прочитать данные с другого сервера
   ![[Pasted image 20250603193552.png]]
4. Если не получилось прочитать данные блок и с другого сервера, тогда в принципе чтение файла зафейлится
   ![[Pasted image 20250603193642.png]]


Процесс записи файла:
1. Создать новый файл в namespaces на NN и определить топологию блоков
   ![[Pasted image 20250603193815.png]]
2. Отправить данные на 1 DN
3. Отправить данные на 2 DN
4. Отправить данные на 3 DN
5. Получить подтверждение Success/Failure от 3 DN
6. Получить подтверждение Success/Failure от 2 DN
7. Получить подтверждение Success/Failure от 1 DN
   ![[Pasted image 20250603194010.png]]









#hdfs #hadoop #big_data #file_systems