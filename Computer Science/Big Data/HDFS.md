## Архитектура HDFS

Основана на архитектуре GFS(Google File System). Является её опен-сорсным аналогом под лицензией Apache.

- Работает на кластере серверов;
- Для пользователя выглядит как один диск, т.е. hdfs сам по себе распределяет, реплецирует и хранит файлы;
- Построена поверх обычных linux файловых систем([[ext4]], [[btrfs]], [[zfs]]).

<mark style="background: #FFF3A3A6;">Fault tolerant</mark>, т.е. данные не теряются если диск или сервер выходит из строя.
Железо, для HDFS - это не суперкомпьютеры, а обычное серверное железо.

HDFS хорошо подходит для того, чтобы хранить файлы по несколько петтабайт, но лучше хранить большие файлы, чем множество маленьких файлов.
HDFS подходит для:
- Паттерн <mark style="background: #FFF3A3A6;">"write once, read many times"</mark>, т.е. лучше создавать логику работы с HDFS таким образом, чтобы записывать в файл данные один раз, а использовать эти данные много раз;
- <mark style="background: #FFF3A3A6;">Оптимизация под последовательное чтение</mark>, т.е. нельзя начать читать файл в произвольном месте, т.к. придется переставлять головку жёсткого диска много раз;
- Опперация append появилась в Hadoop 0.21.
HDFS <mark style="background: #FF5582A6;">не подходит</mark>:
- <mark style="background: #FF5582A6;">Low latency reads</mark>
	- Высокая пропускная способностьь вместо быстрого доступа к данным, т.е. если нам необходимо получать маленькое количество данных очень быстро, то использовать HDFS в данном случае не оптимально, т.к. HDFS сконструирован под чтение больших файлов и обработки больших объёмов данных единовременно;
	- [[Hbase]] частично помогает решить данную задачу.
- <mark style="background: #FF5582A6;">Большое количество маленьких файлов.</mark> HDFS архитектурно устроена таким образом, что лучше хранить один большой файл на 1000МБ, чем 1000 маленьких файлов по 1МБ;
- Многопоточная запись:
	- Один процесс записи на файл. Т.е. нет возможности 2 процессам/потокам писать в один файл;
	- Данные дописываются в конец файла;
	- Нет поддержки записи по смещению.
##  Демоны HDFS

[[Демон]] - это <mark style="background: #FFF3A3A6;">фоновый процесс</mark>, который запускается вместе с запуском системы. Он работает в фоне без взаимодействия с пользователем и <mark style="background: #FFF3A3A6;">обеспечивает</mark> в целом <mark style="background: #FFF3A3A6;">работу системы</mark>.
В HDFS существует 3 типа daemon процессов:
- NameNode
- Secondary NameNode
- DataNode

![[Pasted image 20250603145821.png]]

Распишем про каждый из демонов поконкретнее.

### NameNode

NameNode - <mark style="background: #FFF3A3A6;">это основной процесс в HDFS.</mark> Он запускается на одной выделенной машине. Этот процесс отвечает за:
- файловое пространство(namespace)
- мета-информацию
- расположение блока файлов
<mark style="background: #FFF3A3A6;">NameNode</mark> практически не хранит данные у себя, она <mark style="background: #FFF3A3A6;">хранит только мета-информацию о данных.</mark> Т.е. он хранит информацию, где какие файлы находятся, т.е. собирает информацию о дереве HDFS. Сервер, на котором крутится NameNode должен быть достаточно большим с большим количеством оперативной памяти, т.к. <mark style="background: #FFF3A3A6;">вся информация о файлах хранится в оперативной памяти</mark>. Соответственно, <mark style="background: #FF5582A6;">количество файлов</mark> в кластере напрямую <mark style="background: #FF5582A6;">ограничено количеством оперативной памяти</mark> в NameNode.
NameNode должен быть отказоустойчивым, т.к. при падении NameNode мы теряем возможность взаимодействовать со всем кластером.

### DataNode

DataNode - это демон отвечающий за хранение информации.
- Отдаёт и принимает блоки данных;
- Отправляет отчёт о состоянии на NameNode;
- Запускается на каждой машине кластера.

### Secondary NameNode

NameNode хранит свое состояние в файле <mark style="background: #FF5582A6;">fsimage</mark>. И в дальнейшем в эксплуатации добавляет изменения <mark style="background: #FFF3A3A6;">в log файл</mark>. Проблема в том, что когда падает NameNode, чтобы восстановить работу HDFS необходимо время, чтобы она накатила изменения из log файла. Для этого и существует Secondary NameNode, которая:
- <mark style="background: #FF5582A6;">Периодически обновляет свой fsimage</mark>, т.е. при поднятии не нужно накатывать изменения из log файла;
- Требует то же железо, что и NameNode;
- <mark style="background: #FF5582A6;">(!) Не используется для high-availability, т.е. это не backup для NameNode</mark>.

## Файлы и блоки

<mark style="background: #FFF3A3A6;">Файлы в HDFS состоят из блоков.</mark> Соответственно блок это единица хранения данных. <mark style="background: #FFF3A3A6;">Информация о местонахождении</mark> конкретных блоков, конкретного файла находится на <mark style="background: #FFF3A3A6;">NameNode</mark>. Соответсвенно сами блоки хранятся на DataNode. DataNode не знает частью какого файла является конкретный блок.Чтобы прочитать файл процесс сначала идёт к NameNode, узнаёт из каких блоков состоит файл и где эти блоки находятся. 

### Репликация

Блоки <mark style="background: #FFF3A3A6;">реплицируются</mark> по машинам <mark style="background: #FFF3A3A6;">в процессе записи</mark>:
 - Один и тот же блок хранится на нескольких DataNode;
 - Фактор репликации по умолчанию равен 3;
 - Это необходимо для fault-tolerance и упрощению доступа.

Например, у нас упал один сервер и нам необходимо прочитать какие-то данные с упавшего сервера. Если бы у нас не было репликации, то данные были бы в лучшем случае задержаны для получения, а в худшем случае и вовсе утеряны. <mark style="background: #FFF3A3A6;">Соответственно с помощью репликации мы не теряем данные, даже если упал целый сервер</mark>, потому что у нас есть ещё как минимум 2 таких же блока, которые находятся на других DataNode для каждого из блоков находящихся на упавшем DataNode. 

<mark style="background: #FFF3A3A6;">Так же данные реплики могут быть использованы для упрощения доступа к данным</mark>. Например 2-ум процессам необходим доступ к одним и тем же данным на чтение. Для этого можно использовать <mark style="background: #FFF3A3A6;">одни и те же блоки с разных DataNode,</mark> за счёт чего мы увеличим скорость чтения данных для обоих процессов, т.к. второму процессу не придется ждать пока первый процесс закончится.

<mark style="background: #FFF3A3A6;">Фактор репликации тоже можно варьировать.</mark> Если у нас хранятся особенно важные данные, то можно повысить фактор репликации до 4 или до 5. С этим не стоит переусердствовать, <mark style="background: #FFF3A3A6;">т.к. повышение фактора репликации приводит к понижению количества файлов, которые можно записать в HDFS.</mark>
<mark style="background: #FF5582A6;">Если же у нас хранятся не особенно важные данные,</mark> которые можно восстановить из других источников, то <mark style="background: #FF5582A6;">можно понизить фактор репликации до 2.</mark> До 1 никто обычно не понижает фактор репликации, потому что каждый отказ приводит к потере данных, из-за чего придется часто использовать механизмы восстановления.

![[Pasted image 20250603161127.png]]

<mark style="background: #FFF3A3A6;">Стандартный размер блоков в HDFS 64МБ, 128МБ, 256МБ.</mark>
<mark style="background: #FF5582A6;">Такие большие блоки необходимы для того, чтобы seek time(время перемещения головки диска в место, с которого необходимо начать считывание) был значительно меньше transfer rate(времени считывания).</mark>
На картинке внизу ярко представлена разница в том, что если у нас будет большое кол-во маленьких блоков, мы потеряем время на сдвиги головки и из-за этого увеличиться время чтения одного файла.

![[Pasted image 20250603161515.png]]

<mark style="background: #FFF3A3A6;">Репликацией блоков занимается NameNode.</mark> При этом необходимо балансировать между надёжностью и нагрузкой на сеть:
- Попытка снизить нагрузку на сеть(bandwidth)
- <span style="background:#fff88f">Попытка улучшить надежность путём размещения реплик в разных стойках.</span>
<span style="background:#fff88f">	- Из трёх реплик первую рамещают на DataNode</span>
<span style="background:#fff88f">	- Вторую реплику размещают в той же стойке но на другой DataNode</span>
<span style="background:#fff88f">	- Третью реплику размещают на DataNode в другой стойке</span>
Это делают для того, чтобы если сломается даже вся стойка, данные всё равно можно было бы восстановить.

## Взаимодействие клиента hdfs  и его демонов

Клиентами для демонов могут выступать различные Java/Python программы, так же это может быть shell команды. 
<span style="background:#fff88f">Все действия клиента можно разделить на 2 группы:</span>
- <span style="background:#fff88f">получение данных. Например, в какой директории находится файл, на сколько блоков он разбит, на какие кластера реплецирован и т.д.</span>
- <span style="background:#fff88f">запись данных. Можно менять права доступа к файлу, фактор репликации файла и т.д.</span>
<span style="background:#ff4d4f">Процесс чтения файла:</span>
1. Сначала клиент запрашивает у NameNode расположение блоков B1 и B2
2. После получения информации о местоположении блоков клиент начинает вычитывать данные с DataNode
   ![[Pasted image 20250603193142.png]]
3. Если при чтении произошла ошибка и блок B2 не был прочитан, то, т.к. у нас есть репликация, можно прочитать данные с другого сервера
   ![[Pasted image 20250603193552.png]]
4. Если не получилось прочитать данные блок и с другого сервера, тогда в принципе чтение файла зафейлится
   ![[Pasted image 20250603193642.png]]


Процесс записи файла:
1. Создать новый файл в namespaces на NN и определить топологию блоков
   ![[Pasted image 20250603193815.png]]
2. Отправить данные на 1 DN
3. Отправить данные на 2 DN
4. Отправить данные на 3 DN
5. Получить подтверждение Success/Failure от 3 DN
6. Получить подтверждение Success/Failure от 2 DN
7. Получить подтверждение Success/Failure от 1 DN
   ![[Pasted image 20250603194010.png]]


## NameNode: использование памяти и fault tolerance

### Использование памяти

Вся информация о блоках в <span style="background:#fff88f">hdfs хранится в виде файлов</span> `fsimage` и логов, которые трекируют изменения в hdfs. При этом при взаимодействии с клиентом вся информация берётся <span style="background:#ff4d4f">из оперативной памяти</span>. Это означает, что чем больше кластер и чем больше файлов в нём находится, тем больше необходимо оперативной памяти в NameNode. Соответственно лучше иметь миллион больших файлов, чем миллиард маленьких. Так же можно сказать, что использование одной NameNode для большого кластера может оказаться не эффективным, потому что оперативной памяти может просто не хватить. Для этого придумали подход [[NameNode Federation]], который реализовывает кластер из NameNode.

Ещё раз проговорим про размер блока в файловой системе. Чем больше размер блока, тем меньше блоков в файловой системе, соответственно, чем меньше размер блока, тем больше блоков  в файловой системе, исходя из этого делаем вывод, что нужно грамотно подбирать размер блока, потому что если взять слишком маленький размер блока, можно улететь по Out of Memory.

![[Pasted image 20250605170842.png]]

Рассмотрим на конкретном примере, где у нас есть 200 ТБ:

![[Pasted image 20250605171210.png]]

### Fault tolerance

 <span style="background:#ff4d4f">NameNode - это SPOF(Single point of failure) в HDFS,</span> потому что падение NameNode, приводит к тому, что вся система перестаёт работать. Соответственно она должна работать на отдельной надёжной машине.

<span style="background:#fff88f">В последних версиях hadoop, сделали "правильную" Secondary NameNode(Active Standby), который в случае падения NameNode берёт всю нагрузку на себя.</span> При этом все данные, которые были на NameNode он копирует себе.



## Доступ к HDFS

Для доступа к HDFS есть 2 вида доступа:
- Direct Access
- Proxy Server

### Direct Access

Виды прямого доступа к HDFS:
- API для языков программирования
- Клиент запрашивает метаданные от NameNode
- Клиент запрашивает данные от Data Node
- Используется для Map Reduce
![[Pasted image 20250609154702.png]]

### Proxy Server

Каждый клиент взаимодействует на прямую с прокси сервером:
- Существует несколько серверов в поставке с Hadoop
	- Thrift - язык определения интерфейса
	- WebHDFS REST - ответы в формате JSON, XML или ProtoBuf
	-  Avro - механизм сериализации

<span style="background:#fff88f">Этот подход имеет минусы, потому что прокси сервер может стать bottle neckом,</span> потому что если мы будем передавать большое количество данных, необходимо правильно масштабировать ещё и сам прокси сервер.

#hdfs #hadoop #big_data #file_systems